{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa285b3-3540-4603-96cc-f02a1ecd3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(\"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?).\".encode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0e37413-0ef4-4e88-8b45-082388101baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "length: 23926\n",
      "------\n",
      "length: 25215\n"
     ]
    }
   ],
   "source": [
    "text='''Nathan Reed\n",
    "Blog Stuff I’ve Made Talks About Me\n",
    "The Many Meanings of “Shader”Quadrilateral Interpolation, Part 2\n",
    "A Programmer’s Introduction to Unicode\n",
    "March 3, 2017 · Coding · 25 Comments\n",
    "\n",
    "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
    "\n",
    "A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.\n",
    "\n",
    "I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\n",
    "\n",
    "Diversity and Inherent Complexity\n",
    "The Unicode Codespace\n",
    "Codespace Allocation\n",
    "Scripts\n",
    "Usage Frequency\n",
    "Encodings\n",
    "UTF-8\n",
    "UTF-16\n",
    "Combining Marks\n",
    "Canonical Equivalence\n",
    "Normalization Forms\n",
    "Grapheme Clusters\n",
    "And More…\n",
    "Diversity and Inherent Complexity\n",
    "As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.\n",
    "\n",
    "When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”\n",
    "\n",
    "However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.\n",
    "\n",
    "Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.\n",
    "\n",
    "Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.\n",
    "\n",
    "Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!\n",
    "\n",
    "The Unicode Codespace\n",
    "Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.\n",
    "\n",
    "The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.\n",
    "\n",
    "Codespace Allocation\n",
    "To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.\n",
    "\n",
    "Map of the Unicode codespace (click to zoom)\n",
    "\n",
    "White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.\n",
    "\n",
    "Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.\n",
    "\n",
    "(In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)\n",
    "\n",
    "Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.\n",
    "\n",
    "Scripts\n",
    "Let’s zoom in on the first three planes, since that’s where the action is:\n",
    "\n",
    "Map of scripts in Unicode planes 0–2 (click to zoom)\n",
    "\n",
    "This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.\n",
    "\n",
    "Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).\n",
    "\n",
    "Usage Frequency\n",
    "One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.\n",
    "\n",
    "Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)\n",
    "\n",
    "You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.\n",
    "\n",
    "Encodings\n",
    "We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?\n",
    "\n",
    "The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.\n",
    "\n",
    "Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.\n",
    "\n",
    "Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.\n",
    "\n",
    "UTF-8\n",
    "In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.\n",
    "\n",
    "UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:\n",
    "\n",
    "UTF-8 (binary)\tCode point (binary)\tRange\n",
    "0xxxxxxx\txxxxxxx\tU+0000–U+007F\n",
    "110xxxxx 10yyyyyy\txxxxxyyyyyy\tU+0080–U+07FF\n",
    "1110xxxx 10yyyyyy 10zzzzzz\txxxxyyyyyyzzzzzz\tU+0800–U+FFFF\n",
    "11110xxx 10yyyyyy 10zzzzzz 10wwwwww\txxxyyyyyyzzzzzzwwwwww\tU+10000–U+10FFFF\n",
    "A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.\n",
    "\n",
    "Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.\n",
    "\n",
    "However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.\n",
    "\n",
    "UTF-16\n",
    "The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.\n",
    "\n",
    "Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:\n",
    "\n",
    "UTF-16 (binary)\tCode point (binary)\tRange\n",
    "xxxxxxxxxxxxxxxx\txxxxxxxxxxxxxxxx\tU+0000–U+FFFF\n",
    "110110xxxxxxxxxx 110111yyyyyyyyyy\txxxxxxxxxxyyyyyyyyyy + 0x10000\tU+10000–U+10FFFF\n",
    "A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.\n",
    "\n",
    "Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.\n",
    "\n",
    "Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)\n",
    "\n",
    "By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)\n",
    "\n",
    "Combining Marks\n",
    "In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!\n",
    "\n",
    "Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.\n",
    "\n",
    "In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.\n",
    "\n",
    "If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.\n",
    "\n",
    "For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.\n",
    "\n",
    "Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.\n",
    "\n",
    "Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓\n",
    "\n",
    "A few other places where dynamic character composition shows up in Unicode:\n",
    "\n",
    "Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.\n",
    "\n",
    "A Hebrew example, with niqqud:\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד\n",
    "Normal writing (no niqqud):\tאת דלתי הזיז הניע, קטב לשכתי ישוד\n",
    "Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “​ि” = “हि” (“h” + “i” = “hi”).\n",
    "\n",
    "Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”).\n",
    "\n",
    "Canonical Equivalence\n",
    "In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.\n",
    "\n",
    "Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.\n",
    "\n",
    "For example, the Vietnamese letter “ệ” can be expressed in five different ways:\n",
    "\n",
    "Fully precomposed: U+1EC7 “ệ”\n",
    "Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂”\n",
    "Partially precomposed: U+00EA “ê” + U+0323 “◌̣”\n",
    "Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂”\n",
    "Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣”\n",
    "Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!\n",
    "\n",
    "Normalization Forms\n",
    "To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).\n",
    "\n",
    "The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)\n",
    "\n",
    "The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).\n",
    "\n",
    "There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.\n",
    "\n",
    "Grapheme Clusters\n",
    "As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.\n",
    "\n",
    "UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.\n",
    "\n",
    "The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.\n",
    "\n",
    "Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.\n",
    "\n",
    "And More…\n",
    "There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.\n",
    "\n",
    "Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.\n",
    "\n",
    "Further reading:\n",
    "\n",
    "The Unicode Standard\n",
    "UTF-8 Everywhere Manifesto\n",
    "Dark corners of Unicode by Eevee\n",
    "ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things\n",
    "Python 3 Unicode Howto\n",
    "Google Noto Fonts—set of fonts intended to cover all assigned code points\n",
    "The Many Meanings of “Shader”Quadrilateral Interpolation, Part 2\n",
    "25 Comments on “A Programmer’s Introduction to Unicode”\n",
    "\n",
    "\n",
    " \n",
    "Subscribe\n",
    "RSS RSS\n",
    "Recent Posts\n",
    "Reading Veach’s Thesis, Part 2\n",
    "Reading Veach’s Thesis\n",
    "Texture Gathers and Coordinate Precision\n",
    "git-partial-submodule\n",
    "Slope Space in BRDF Theory\n",
    "Hash Functions for GPU Rendering\n",
    "All Posts\n",
    "Categories\n",
    "Graphics(32)\n",
    "Coding(23)\n",
    "Math(21)\n",
    "GPU(15)\n",
    "Physics(6)\n",
    "Eye Candy(4)\n",
    "© 2007–2024 by Nathan Reed. Licensed CC-BY-4.0.'''\n",
    "tokens=text.encode(\"utf-8\")\n",
    "tokens=list(map(int,tokens))\n",
    "print(\"------\")\n",
    "# print(text)\n",
    "print(\"length:\", len(text))\n",
    "print(\"------\")\n",
    "# print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0642d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts={}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair]=counts.get(pair,0)+1\n",
    "    return counts\n",
    "\n",
    "stats=get_stats(tokens)\n",
    "print (stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb751fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(((v,k) for k,v in stats.items()),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0715180f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('â', '\\x80')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(226), chr(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d22654b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair=max(stats, key= stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    i=0\n",
    "    newids=[]\n",
    "    while i < len(ids):\n",
    "        if i < len(ids)-1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i+=1\n",
    "    return newids\n",
    "# print (merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "token2=merge(tokens, top_pair, 256)\n",
    "print(token2)\n",
    "print(\"Length:\", len(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "112d6352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into new token 256\n",
      "merging (105, 110) into new token 257\n",
      "merging (115, 32) into new token 258\n",
      "merging (116, 104) into new token 259\n",
      "merging (101, 114) into new token 260\n",
      "merging (99, 111) into new token 261\n",
      "merging (116, 32) into new token 262\n",
      "merging (226, 128) into new token 263\n",
      "merging (44, 32) into new token 264\n",
      "merging (97, 110) into new token 265\n",
      "merging (111, 114) into new token 266\n",
      "merging (100, 32) into new token 267\n",
      "merging (97, 114) into new token 268\n",
      "merging (101, 110) into new token 269\n",
      "merging (257, 103) into new token 270\n",
      "merging (261, 100) into new token 271\n",
      "merging (121, 32) into new token 272\n",
      "merging (97, 108) into new token 273\n",
      "merging (111, 110) into new token 274\n",
      "merging (259, 256) into new token 275\n",
      "merging (116, 105) into new token 276\n",
      "merging (111, 32) into new token 277\n",
      "merging (97, 99) into new token 278\n",
      "merging (101, 115) into new token 279\n",
      "merging (102, 32) into new token 280\n",
      "merging (112, 111) into new token 281\n",
      "merging (271, 256) into new token 282\n",
      "merging (270, 32) into new token 283\n",
      "merging (46, 32) into new token 284\n",
      "merging (111, 280) into new token 285\n",
      "merging (101, 267) into new token 286\n",
      "merging (116, 260) into new token 287\n",
      "merging (116, 277) into new token 288\n",
      "merging (257, 32) into new token 289\n",
      "merging (97, 32) into new token 290\n",
      "merging (115, 116) into new token 291\n",
      "merging (266, 32) into new token 292\n",
      "merging (111, 117) into new token 293\n",
      "merging (99, 104) into new token 294\n",
      "merging (120, 120) into new token 295\n",
      "merging (101, 120) into new token 296\n",
      "merging (263, 156) into new token 297\n",
      "merging (263, 157) into new token 298\n",
      "merging (101, 258) into new token 299\n",
      "merging (265, 267) into new token 300\n",
      "merging (105, 116) into new token 301\n",
      "merging (261, 109) into new token 302\n",
      "merging (112, 108) into new token 303\n",
      "merging (114, 101) into new token 304\n",
      "merging (281, 257) into new token 305\n",
      "merging (10, 10) into new token 306\n",
      "merging (263, 153) into new token 307\n",
      "merging (110, 105) into new token 308\n",
      "merging (97, 116) into new token 309\n",
      "merging (273, 108) into new token 310\n",
      "merging (260, 32) into new token 311\n",
      "merging (105, 258) into new token 312\n",
      "merging (282, 305) into new token 313\n",
      "merging (276, 274) into new token 314\n",
      "merging (85, 308) into new token 315\n",
      "merging (97, 258) into new token 316\n",
      "merging (100, 105) into new token 317\n",
      "merging (111, 119) into new token 318\n",
      "merging (117, 115) into new token 319\n",
      "merging (116, 258) into new token 320\n",
      "merging (114, 105) into new token 321\n",
      "merging (115, 105) into new token 322\n",
      "merging (115, 101) into new token 323\n",
      "merging (114, 111) into new token 324\n",
      "merging (97, 109) into new token 325\n",
      "merging (97, 262) into new token 326\n",
      "merging (298, 32) into new token 327\n",
      "merging (115, 264) into new token 328\n",
      "merging (46, 306) into new token 329\n",
      "merging (84, 104) into new token 330\n",
      "merging (315, 282) into new token 331\n",
      "merging (108, 108) into new token 332\n",
      "merging (121, 121) into new token 333\n",
      "merging (265, 32) into new token 334\n",
      "merging (108, 101) into new token 335\n",
      "merging (294, 268) into new token 336\n",
      "merging (102, 292) into new token 337\n",
      "merging (278, 287) into new token 338\n",
      "merging (97, 98) into new token 339\n",
      "merging (336, 338) into new token 340\n",
      "merging (85, 84) into new token 341\n",
      "merging (341, 70) into new token 342\n",
      "merging (119, 105) into new token 343\n",
      "merging (97, 112) into new token 344\n",
      "merging (342, 45) into new token 345\n",
      "merging (85, 43) into new token 346\n",
      "merging (273, 32) into new token 347\n",
      "merging (268, 256) into new token 348\n",
      "merging (259, 326) into new token 349\n",
      "merging (105, 99) into new token 350\n",
      "merging (115, 117) into new token 351\n",
      "merging (119, 104) into new token 352\n",
      "merging (101, 109) into new token 353\n",
      "merging (97, 115) into new token 354\n",
      "merging (32, 297) into new token 355\n",
      "merging (32, 285) into new token 356\n",
      "merging (111, 109) into new token 357\n",
      "merging (105, 262) into new token 358\n",
      "merging (118, 260) into new token 359\n",
      "merging (291, 114) into new token 360\n",
      "merging (101, 264) into new token 361\n",
      "merging (108, 256) into new token 362\n",
      "merging (48, 48) into new token 363\n",
      "merging (97, 314) into new token 364\n",
      "merging (307, 258) into new token 365\n",
      "merging (276, 99) into new token 366\n",
      "merging (97, 103) into new token 367\n",
      "merging (261, 110) into new token 368\n",
      "merging (101, 108) into new token 369\n",
      "merging (295, 295) into new token 370\n",
      "merging (98, 256) into new token 371\n",
      "merging (294, 32) into new token 372\n",
      "merging (269, 32) into new token 373\n",
      "merging (259, 32) into new token 374\n",
      "merging (113, 117) into new token 375\n",
      "merging (114, 279) into new token 376\n",
      "merging (313, 320) into new token 377\n",
      "merging (116, 296) into new token 378\n",
      "merging (332, 32) into new token 379\n",
      "merging (265, 272) into new token 380\n",
      "merging (263, 148) into new token 381\n",
      "merging (279, 264) into new token 382\n",
      "merging (274, 32) into new token 383\n",
      "merging (49, 54) into new token 384\n",
      "merging (98, 257) into new token 385\n",
      "merging (310, 272) into new token 386\n",
      "merging (284, 330) into new token 387\n",
      "merging (302, 281) into new token 388\n",
      "merging (121, 293) into new token 389\n",
      "merging (97, 100) into new token 390\n",
      "merging (278, 321) into new token 391\n",
      "merging (391, 366) into new token 392\n",
      "merging (103, 114) into new token 393\n",
      "merging (32, 288) into new token 394\n",
      "merging (103, 104) into new token 395\n",
      "merging (102, 105) into new token 396\n",
      "merging (345, 56) into new token 397\n",
      "merging (266, 109) into new token 398\n",
      "merging (260, 256) into new token 399\n",
      "merging (313, 116) into new token 400\n",
      "merging (269, 271) into new token 401\n",
      "merging (98, 121) into new token 402\n",
      "merging (317, 392) into new token 403\n",
      "merging (269, 116) into new token 404\n",
      "merging (119, 266) into new token 405\n",
      "merging (265, 103) into new token 406\n",
      "merging (259, 311) into new token 407\n",
      "merging (70, 70) into new token 408\n",
      "merging (100, 101) into new token 409\n",
      "merging (109, 266) into new token 410\n",
      "merging (269, 262) into new token 411\n",
      "merging (112, 101) into new token 412\n",
      "merging (117, 108) into new token 413\n",
      "merging (112, 304) into new token 414\n",
      "merging (402, 116) into new token 415\n",
      "merging (122, 122) into new token 416\n",
      "merging (108, 105) into new token 417\n",
      "merging (112, 278) into new token 418\n",
      "merging (112, 376) into new token 419\n",
      "merging (49, 49) into new token 420\n",
      "merging (117, 114) into new token 421\n",
      "merging (73, 110) into new token 422\n",
      "merging (112, 324) into new token 423\n",
      "merging (310, 32) into new token 424\n",
      "merging (343, 374) into new token 425\n",
      "merging (269, 99) into new token 426\n",
      "merging (115, 277) into new token 427\n",
      "merging (117, 110) into new token 428\n",
      "merging (115, 284) into new token 429\n",
      "merging (204, 173) into new token 430\n",
      "merging (205, 153) into new token 431\n",
      "merging (204, 178) into new token 432\n",
      "merging (118, 256) into new token 433\n",
      "merging (293, 262) into new token 434\n",
      "merging (360, 270) into new token 435\n",
      "merging (307, 262) into new token 436\n",
      "merging (116, 97) into new token 437\n",
      "merging (109, 97) into new token 438\n",
      "merging (289, 275) into new token 439\n",
      "merging (335, 116) into new token 440\n",
      "merging (101, 105) into new token 441\n",
      "merging (333, 333) into new token 442\n",
      "merging (115, 286) into new token 443\n",
      "merging (111, 100) into new token 444\n",
      "merging (240, 159) into new token 445\n",
      "merging (318, 32) into new token 446\n",
      "merging (117, 262) into new token 447\n",
      "merging (99, 334) into new token 448\n",
      "merging (110, 111) into new token 449\n",
      "merging (410, 256) into new token 450\n",
      "merging (105, 108) into new token 451\n",
      "merging (121, 264) into new token 452\n",
      "merging (99, 108) into new token 453\n",
      "merging (440, 287) into new token 454\n",
      "merging (302, 385) into new token 455\n",
      "merging (403, 258) into new token 456\n",
      "merging (204, 171) into new token 457\n",
      "merging (204, 177) into new token 458\n",
      "merging (101, 100) into new token 459\n",
      "merging (315, 271) into new token 460\n",
      "merging (32, 300) into new token 461\n",
      "merging (32, 115) into new token 462\n",
      "merging (265, 100) into new token 463\n",
      "merging (259, 312) into new token 464\n",
      "merging (269, 100) into new token 465\n",
      "merging (279, 418) into new token 466\n",
      "merging (115, 10) into new token 467\n",
      "merging (345, 384) into new token 468\n",
      "merging (389, 32) into new token 469\n",
      "merging (117, 109) into new token 470\n",
      "merging (303, 256) into new token 471\n",
      "merging (98, 272) into new token 472\n",
      "merging (108, 272) into new token 473\n",
      "merging (45, 98) into new token 474\n",
      "merging (205, 136) into new token 475\n",
      "merging (204, 185) into new token 476\n",
      "merging (258, 285) into new token 477\n",
      "merging (118, 311) into new token 478\n",
      "merging (115, 357) into new token 479\n",
      "merging (268, 107) into new token 480\n",
      "merging (316, 290) into new token 481\n",
      "merging (110, 101) into new token 482\n",
      "merging (105, 109) into new token 483\n",
      "merging (108, 406) into new token 484\n",
      "merging (484, 117) into new token 485\n",
      "merging (485, 367) into new token 486\n",
      "merging (352, 105) into new token 487\n",
      "merging (487, 372) into new token 488\n",
      "merging (263, 147) into new token 489\n",
      "merging (32, 346) into new token 490\n",
      "merging (205, 150) into new token 491\n",
      "merging (204, 187) into new token 492\n",
      "merging (205, 135) into new token 493\n",
      "merging (204, 188) into new token 494\n",
      "merging (204, 164) into new token 495\n",
      "merging (204, 166) into new token 496\n",
      "merging (393, 325) into new token 497\n",
      "merging (351, 112) into new token 498\n",
      "merging (112, 266) into new token 499\n",
      "merging (107, 256) into new token 500\n",
      "merging (283, 109) into new token 501\n",
      "merging (378, 262) into new token 502\n",
      "merging (105, 115) into new token 503\n",
      "merging (99, 321) into new token 504\n",
      "merging (375, 426) into new token 505\n",
      "merging (278, 104) into new token 506\n",
      "merging (313, 262) into new token 507\n",
      "merging (474, 358) into new token 508\n",
      "merging (397, 32) into new token 509\n",
      "merging (204, 176) into new token 510\n",
      "merging (204, 179) into new token 511\n",
      "merging (204, 153) into new token 512\n",
      "merging (204, 186) into new token 513\n",
      "merging (204, 158) into new token 514\n",
      "merging (108, 111) into new token 515\n",
      "merging (330, 256) into new token 516\n",
      "merging (100, 111) into new token 517\n",
      "merging (99, 105) into new token 518\n",
      "merging (339, 434) into new token 519\n",
      "merging (262, 285) into new token 520\n",
      "merging (273, 105) into new token 521\n",
      "merging (309, 256) into new token 522\n",
      "merging (504, 112) into new token 523\n",
      "merging (116, 283) into new token 524\n",
      "merging (291, 266) into new token 525\n",
      "merging (101, 45) into new token 526\n",
      "merging (116, 111) into new token 527\n",
      "merging (401, 270) into new token 528\n",
      "merging (414, 388) into new token 529\n",
      "merging (204, 160) into new token 530\n",
      "merging (204, 170) into new token 531\n",
      "merging (205, 141) into new token 532\n",
      "merging (327, 43) into new token 533\n",
      "merging (497, 109) into new token 534\n",
      "merging (284, 73) into new token 535\n",
      "merging (286, 288) into new token 536\n",
      "merging (479, 256) into new token 537\n",
      "merging (109, 32) into new token 538\n",
      "merging (118, 105) into new token 539\n",
      "merging (340, 32) into new token 540\n",
      "merging (378, 116) into new token 541\n",
      "merging (101, 359) into new token 542\n",
      "merging (301, 272) into new token 543\n",
      "merging (273, 427) into new token 544\n",
      "merging (274, 256) into new token 545\n",
      "merging (110, 436) into new token 546\n",
      "merging (387, 256) into new token 547\n",
      "merging (102, 102) into new token 548\n",
      "merging (98, 447) into new token 549\n",
      "merging (268, 105) into new token 550\n",
      "merging (271, 466) into new token 551\n",
      "merging (101, 506) into new token 552\n",
      "merging (303, 265) into new token 553\n",
      "merging (117, 116) into new token 554\n",
      "merging (102, 398) into new token 555\n",
      "merging (117, 112) into new token 556\n",
      "merging (49, 48) into new token 557\n",
      "merging (51, 50) into new token 558\n",
      "merging (205, 133) into new token 559\n",
      "merging (204, 150) into new token 560\n",
      "merging (205, 147) into new token 561\n",
      "merging (204, 152) into new token 562\n",
      "merging (204, 169) into new token 563\n",
      "merging (204, 163) into new token 564\n",
      "merging (205, 148) into new token 565\n",
      "merging (101, 10) into new token 566\n",
      "merging (110, 325) into new token 567\n",
      "merging (395, 262) into new token 568\n",
      "merging (498, 499) into new token 569\n",
      "merging (301, 258) into new token 570\n",
      "merging (73, 32) into new token 571\n",
      "merging (102, 324) into new token 572\n",
      "merging (107, 32) into new token 573\n",
      "merging (465, 260) into new token 574\n",
      "merging (260, 411) into new token 575\n",
      "merging (344, 104) into new token 576\n",
      "merging (304, 419) into new token 577\n",
      "merging (264, 300) into new token 578\n",
      "merging (258, 288) into new token 579\n",
      "merging (115, 121) into new token 580\n",
      "merging (302, 112) into new token 581\n",
      "merging (100, 311) into new token 582\n",
      "merging (256, 275) into new token 583\n",
      "merging (319, 286) into new token 584\n",
      "merging (296, 325) into new token 585\n",
      "merging (119, 119) into new token 586\n",
      "merging (360, 283) into new token 587\n",
      "merging (468, 32) into new token 588\n",
      "merging (455, 501) into new token 589\n",
      "merging (205, 149) into new token 590\n",
      "merging (204, 157) into new token 591\n",
      "merging (204, 159) into new token 592\n",
      "merging (204, 172) into new token 593\n",
      "merging (205, 137) into new token 594\n",
      "merging (32, 215) into new token 595\n",
      "merging (108, 97) into new token 596\n",
      "merging (534, 260) into new token 597\n",
      "merging (99, 314) into new token 598\n",
      "merging (67, 444) into new token 599\n",
      "merging (448, 371) into new token 600\n",
      "merging (102, 257) into new token 601\n",
      "merging (422, 32) into new token 602\n",
      "merging (449, 262) into new token 603\n",
      "merging (114, 574) into new token 604\n",
      "merging (521, 122) into new token 605\n",
      "merging (364, 32) into new token 606\n",
      "merging (111, 115) into new token 607\n",
      "merging (98, 101) into new token 608\n",
      "merging (103, 101) into new token 609\n",
      "merging (576, 353) into new token 610\n",
      "merging (610, 256) into new token 611\n",
      "merging (319, 287) into new token 612\n",
      "merging (110, 267) into new token 613\n",
      "merging (266, 101) into new token 614\n",
      "merging (302, 303) into new token 615\n",
      "merging (65, 83) into new token 616\n",
      "merging (616, 67) into new token 617\n",
      "merging (617, 73) into new token 618\n",
      "merging (301, 365) into new token 619\n",
      "merging (291, 353) into new token 620\n",
      "merging (115, 523) into new token 621\n",
      "merging (115, 270) into new token 622\n",
      "merging (622, 362) into new token 623\n",
      "merging (413, 276) into new token 624\n",
      "merging (114, 32) into new token 625\n",
      "merging (99, 310) into new token 626\n",
      "merging (626, 286) into new token 627\n",
      "merging (111, 407) into new token 628\n",
      "merging (112, 260) into new token 629\n",
      "merging (105, 103) into new token 630\n",
      "merging (585, 303) into new token 631\n",
      "merging (631, 361) into new token 632\n",
      "merging (119, 97) into new token 633\n",
      "merging (323, 505) into new token 634\n",
      "merging (296, 419) into new token 635\n",
      "merging (322, 314) into new token 636\n",
      "merging (354, 256) into new token 637\n",
      "merging (589, 480) into new token 638\n",
      "merging (204, 151) into new token 639\n",
      "merging (204, 174) into new token 640\n",
      "merging (204, 156) into new token 641\n",
      "merging (204, 175) into new token 642\n",
      "merging (65, 32) into new token 643\n",
      "merging (100, 117) into new token 644\n",
      "merging (102, 101) into new token 645\n",
      "merging (257, 288) into new token 646\n",
      "merging (460, 101) into new token 647\n",
      "merging (111, 102) into new token 648\n",
      "merging (109, 105) into new token 649\n",
      "merging (103, 111) into new token 650\n",
      "merging (72, 318) into new token 651\n",
      "merging (264, 292) into new token 652\n",
      "merging (41, 32) into new token 653\n",
      "merging (101, 329) into new token 654\n",
      "merging (618, 571) into new token 655\n",
      "merging (106, 319) into new token 656\n",
      "merging (656, 262) into new token 657\n",
      "merging (269, 103) into new token 658\n",
      "merging (119, 114) into new token 659\n",
      "merging (104, 105) into new token 660\n",
      "merging (97, 118) into new token 661\n",
      "merging (259, 441) into new token 662\n",
      "merging (662, 625) into new token 663\n",
      "merging (346, 363) into new token 664\n",
      "merging (354, 322) into new token 665\n",
      "merging (400, 328) into new token 666\n",
      "merging (66, 77) into new token 667\n",
      "merging (667, 80) into new token 668\n",
      "merging (32, 40) into new token 669\n",
      "merging (70, 292) into new token 670\n",
      "merging (387, 312) into new token 671\n",
      "merging (259, 399) into new token 672\n",
      "merging (279, 256) into new token 673\n",
      "merging (420, 48) into new token 674\n",
      "merging (416, 416) into new token 675\n",
      "merging (370, 370) into new token 676\n",
      "merging (205, 142) into new token 677\n",
      "merging (215, 153) into new token 678\n",
      "merging (67, 357) into new token 679\n",
      "merging (33, 32) into new token 680\n",
      "merging (445, 133) into new token 681\n",
      "merging (445, 135) into new token 682\n",
      "merging (423, 597) into new token 683\n",
      "merging (116, 119) into new token 684\n",
      "merging (417, 500) into new token 685\n",
      "merging (268, 272) into new token 686\n",
      "merging (115, 276) into new token 687\n",
      "merging (101, 118) into new token 688\n",
      "merging (257, 116) into new token 689\n",
      "merging (572, 538) into new token 690\n",
      "merging (283, 288) into new token 691\n",
      "merging (111, 99) into new token 692\n",
      "merging (651, 542) into new token 693\n",
      "merging (693, 264) into new token 694\n",
      "merging (350, 347) into new token 695\n",
      "merging (118, 273) into new token 696\n",
      "merging (117, 100) into new token 697\n",
      "merging (268, 103) into new token 698\n",
      "merging (293, 395) into new token 699\n",
      "merging (115, 412) into new token 700\n",
      "merging (279, 115) into new token 701\n",
      "merging (268, 121) into new token 702\n",
      "merging (293, 108) into new token 703\n",
      "merging (97, 105) into new token 704\n",
      "merging (114, 256) into new token 705\n",
      "merging (659, 301) into new token 706\n",
      "merging (309, 286) into new token 707\n",
      "merging (258, 289) into new token 708\n",
      "merging (317, 548) into new token 709\n",
      "merging (709, 575) into new token 710\n",
      "merging (312, 290) into new token 711\n",
      "merging (256, 285) into new token 712\n",
      "merging (318, 110) into new token 713\n",
      "merging (624, 471) into new token 714\n",
      "merging (104, 318) into new token 715\n",
      "merging (344, 303) into new token 716\n",
      "merging (343, 379) into new token 717\n",
      "merging (322, 98) into new token 718\n",
      "merging (48, 51) into new token 719\n",
      "merging (281, 115) into new token 720\n",
      "merging (551, 256) into new token 721\n",
      "merging (274, 473) into new token 722\n",
      "merging (356, 275) into new token 723\n",
      "merging (665, 103) into new token 724\n",
      "merging (724, 110) into new token 725\n",
      "merging (110, 277) into new token 726\n",
      "merging (104, 446) into new token 727\n",
      "merging (109, 344) into new token 728\n",
      "merging (59, 32) into new token 729\n",
      "merging (552, 32) into new token 730\n",
      "merging (553, 299) into new token 731\n",
      "merging (41, 306) into new token 732\n",
      "merging (351, 114) into new token 733\n",
      "merging (119, 316) into new token 734\n",
      "merging (102, 266) into new token 735\n",
      "merging (121, 45) into new token 736\n",
      "merging (670, 632) into new token 737\n",
      "merging (401, 283) into new token 738\n",
      "merging (405, 100) into new token 739\n",
      "merging (115, 325) into new token 740\n",
      "merging (108, 658) into new token 741\n",
      "merging (393, 611) into new token 742\n",
      "merging (742, 453) into new token 743\n",
      "merging (743, 612) into new token 744\n",
      "merging (115, 329) into new token 745\n",
      "merging (355, 226) into new token 746\n",
      "merging (746, 151) into new token 747\n",
      "merging (747, 140) into new token 748\n",
      "merging (355, 225) into new token 749\n",
      "merging (529, 443) into new token 750\n",
      "merging (204, 165) into new token 751\n",
      "merging (205, 156) into new token 752\n",
      "merging (204, 168) into new token 753\n",
      "merging (78, 70) into new token 754\n",
      "merging (116, 117) into new token 755\n",
      "merging (73, 307) into new token 756\n",
      "merging (268, 262) into new token 757\n",
      "merging (239, 189) into new token 758\n",
      "merging (263, 140) into new token 759\n",
      "merging (759, 682) into new token 760\n",
      "merging (119, 256) into new token 761\n",
      "merging (319, 283) into new token 762\n",
      "merging (284, 66) into new token 763\n",
      "merging (290, 108) into new token 764\n",
      "merging (100, 309) into new token 765\n",
      "merging (687, 379) into new token 766\n",
      "merging (99, 101) into new token 767\n",
      "merging (119, 32) into new token 768\n",
      "merging (257, 287) into new token 769\n",
      "merging (118, 286) into new token 770\n",
      "merging (405, 107) into new token 771\n",
      "merging (258, 300) into new token 772\n",
      "merging (299, 285) into new token 773\n",
      "merging (301, 121) into new token 774\n",
      "merging (265, 274) into new token 775\n",
      "merging (32, 289) into new token 776\n",
      "merging (615, 296) into new token 777\n",
      "merging (111, 478) into new token 778\n",
      "merging (117, 372) into new token 779\n",
      "merging (110, 470) into new token 780\n",
      "merging (780, 98) into new token 781\n",
      "merging (699, 32) into new token 782\n",
      "merging (649, 568) into new token 783\n",
      "merging (270, 264) into new token 784\n",
      "merging (269, 276) into new token 785\n",
      "merging (580, 620) into new token 786\n",
      "merging (293, 613) into new token 787\n",
      "merging (284, 65) into new token 788\n",
      "merging (116, 373) into new token 789\n",
      "merging (259, 260) into new token 790\n",
      "merging (98, 111) into new token 791\n",
      "merging (577, 404) into new token 792\n",
      "merging (116, 114) into new token 793\n",
      "merging (352, 399) into new token 794\n",
      "merging (302, 109) into new token 795\n",
      "merging (97, 433) into new token 796\n",
      "merging (266, 582) into new token 797\n",
      "merging (718, 362) into new token 798\n",
      "merging (101, 116) into new token 799\n"
     ]
    }
   ],
   "source": [
    "vocab_size=800\n",
    "num_merges=vocab_size-256\n",
    "ids=list(tokens)\n",
    "\n",
    "merges={}\n",
    "for i in range(num_merges):\n",
    "    stats=get_stats(ids)\n",
    "    pair=max(stats, key=stats.get)\n",
    "    idx=256+i\n",
    "    print(f\"merging {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair]= idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e45484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length: 25215\n",
      "ids length: 9433\n",
      "compression ratio: 2.67X\n"
     ]
    }
   ],
   "source": [
    "print(\"Token length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "877c9a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab={idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx]=vocab[p0]+vocab[p1]\n",
    "def decode (ids):\n",
    "    tokens= b\"\".join (vocab[idx] for idx in ids)\n",
    "    text =tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    # print(text)\n",
    "    return text\n",
    "decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9b13690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 332, 277, 87, 266, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens=list(text.encode(\"utf-8\"))\n",
    "    while len(tokens)>2:\n",
    "        stats=get_stats(tokens)\n",
    "        pair=min(stats, key=lambda p:merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx=merges[pair]\n",
    "        tokens=merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0af48805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4718f4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = (decode(encode(text)))\n",
    "print(text == text2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
